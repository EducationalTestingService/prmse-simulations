# PRMSE paper

This is a repository containing the code for the PRMSE paper.

## Paper

The paper draft is on [Overleaf](https://www.overleaf.com/3645379311byhfvpvjczht).

## Getting Started

1. Create a conda environment called by running `conda env create -f environment.yml`. This command might take a few minutes. Please wait for it to finish.

2. Activate this new environment by running `conda activate prmse`.

3. Check out the paper source from overleaf in the directory of your choice. You can do this in two ways: 
    - Downloading directly from the Overleaf GUI (Menu -> Download -> Source) and then uncompressing the downloaded zip file
    - Cloning via git: `git clone https://git.overleaf.com/5e47136080724900019c0cf0`. You'll need your overleaf username and password.

4. Create a symlink named `paper_overleaf` to the directory with the paper source. This symlink should be at the same level as this README file in your working copy. By doing this, the figures generated by the code will be available directly to the paper.

## Analyses

### Data

The raw data for the paper is stored in [this](https://etsorg1-my.sharepoint.com/:f:/g/personal/aloukina_ets_org/EpfEwBs2FT5Gpvfive_L3GQBrB4fV3Z5eyjve4luKarxwA?e=pf7bkg) OneDrive folder.

### Simulations

The code for the simulations is divided into notebooks.  

1. [`notebooks/making_a_dataset.ipynb`](notebooks/making_a_dataset.ipynb). This is the notebook used to create a simulated dataset using the dataset parameters stored in [`notebooks/dataset.json`](notebooks/dataset.json). In addition to creating the dataset, it also contains some preliminary analyses on the dataset to make sure that it behaves as expected. This notebook serializes the dataset and saves it under [`data/default.detaset`](data/default.dataset). This serialized dataset file is then used by the subsequent notebooks to load the dataset. Therefore, changing the parameters in `dataset.json` and re-running this notebook will change the results of the analyses in the other notebooks.

2. [`notebooks/multiple_raters_true_score.ipynb`](notebooks/multiple_raters_true_score.ipynb).  In this notebook, we explore the impact of using a larger number of human raters in the evaluation process. More specifically, we show that as we use more and more human raters, the average of the scores assigned by said raters approaches the true score. In addition, we show that when evaluating a given automated system against an increasing number of human raters, the values of the conventional agreement metrics approach values that would be computed if that same system were to be evaluated against the true score.

3. [`notebooks/metric_stability.ipynb`](notebooks/metric_stability.ipynb). In this notebook, we compare the stability of conventional agreement metrics such as Pearson's correlation, quadratically-weighted kappa, mean squared error, and R^2 to that of the proposed PRMSE metric. We do this by showing that the usual agreement metrics can give very different results depending on the pair of human raters that are used as the reference against which the system score is evaluated. However, the PRMSE metric yields stable evaluation results across different pairs of human raters.

4. [`notebooks/ranking_multiple_systems.ipynb`](notebooks/ranking_multiple_systems.ipynb). In this notebook, we explore how to rank multiple automated scoring systems. Specifically, we consider the situation where we have scores from multiple different automated scoring systems, each with different levels of performance.  We evaluate these systems against the same as well as different pairs of raters and show that while all metrics can rank the systems accurately when using a single rater pair for evaluation, only PRMSE can do the same when a different rater pair is used for every system.

5. [`notebooks/prmse_and_double_scoring.ipynb`](notebooks/prmse_and_double_scoring.ipynb). In this notebook, we explore the impact of the number of double-scored responses on PRMSE. We know that in order to compute PRMSE, we need at least some of the responses to have scores from two human raters. However, it may not be practical to have every single response double-scored. In this notebook, we examine how PRMSE depends on the number of double-scored responses that may be available in the dataset.

### Notes

1. Note that the structure and order of the notebooks does not necessarily follow the order of analyses in the paper. For example, in the paper we first show the gaps in traditional metrics and then demonstrate that PRMSE can help address those. However, in the notebooks, it is more efficient to keep the analyses with and without PRMSE in the same notebook as long as they use the same data. 

2. For efficiency and readability reasons, a lot of code shared by the notebooks is factored out into a package called `simulation` found under [`notebooks/simulation`](notebooks/simulation). This package contains two main Python files:

    - [`notebooks/simulation/dataset.py`](notebooks/simulation/dataset.py). This module contains the main ``Dataset`` class representing the simulated dataset underlying all of the PRMSE simulations.

    - [`notebooks/simulation/utils.py`](notebooks/simulation/dataset.py). This module contains several utility functions needed for the various simulations in the notebooks.

3. Running the [`notebooks/making_a_dataset.ipynb`](notebooks/making_a_dataset.ipynb) also saves three CSV files, one for each of the data frames that can be obtained by calling the `to_frames()` method on the dataset instance saved in `data/default.dataset`. Between themsleves, these 3 CSV files contain all of the simulated scores as well as the rater and system metadata. For a detailed description of each data frame, see the docstring for the `to_frames()` method of the [`Dataset`](notebooks/simulation/dataset.py) class. We make these CSV files available so that they can be examined and modified in other programs such as Excel and R. However, note that making changes to these CSV files will _not_ affect any analyses in any of the other notebooks as they use the `data/default.dataset` file and not these CSV files.
