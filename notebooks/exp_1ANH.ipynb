{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One automated score, many raters\n",
    "\n",
    "This notebooks looks at one automated score evaluated against multiple human raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from rsmtool.analyzer import Analyzer\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('settings.json'))\n",
    "total_n_raters = sum(config['n_raters'])\n",
    "fig_dir = Path('../paper_overleaf/figures')\n",
    "df = pd.read_csv('../data/data.csv')\n",
    "df_raters = pd.read_csv('../data/raters.csv')\n",
    "df_systems = pd.read_csv('../data/systems.csv')\n",
    "metrics_dict = json.load(open('metrics_dict.json'))\n",
    "df['system'] = df[config['sample_system']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cumulative mean of human scores per category.\n",
    "agg_list = []\n",
    "diff_list = []\n",
    "for category in df_raters['category'].unique():\n",
    "    human_columns_category = df_raters[df_raters['category']==category]['rater_id']\n",
    "    df_agg = df[human_columns_category].expanding(min_periods=1, axis=1).mean()\n",
    "    df_agg.columns = ['N={}'.format(n) for n in range(1, len(human_columns_category)+1)]\n",
    "    df_difference = df_agg.subtract(df['true'], axis=0)\n",
    "    df_agg['category'] = category\n",
    "    df_difference['category'] = category\n",
    "    df_agg['id'] = df['id']\n",
    "    agg_list.append(df_agg)\n",
    "    diff_list.append(df_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = pd.concat(agg_list)\n",
    "df_diff = pd.concat(diff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11e6f2510>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Plot difference for key N raters: the plot shows that the scores get closer to true score as the \n",
    "# number of raters increases\n",
    "sns.boxplot(data=df_diff[['N={}'.format(i) for i in config['key_steps_n_raters']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between human score and true score\n",
    "corr_true = df_agg.groupby('category').corrwith(df['true']).unstack().reset_index()\n",
    "corr_true.columns = ['N_raters', 'category', 'correlation']\n",
    "corr_true['N'] = corr_true['N_raters'].apply(lambda x: int(x.split('=')[1]))\n",
    "ax = sns.scatterplot(x='N', y='correlation', hue='category', data=corr_true)\n",
    "ax.axhline(1, color='red')\n",
    "ax.set_xlabel(\"N raters\")\n",
    "ax.set_ylabel(\"Correlation\")\n",
    "ax.set_title(\"Correlation between average human score and true score\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation with a sample system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kappa            0.605565\n",
       "QWK              0.908822\n",
       "exact_agr        0.000000\n",
       "adj_agr         99.750000\n",
       "SMD              0.001948\n",
       "r                0.912429\n",
       "R2               0.799869\n",
       "RMSE             0.332323\n",
       "sys_min          0.944025\n",
       "sys_max          6.633447\n",
       "sys_mean         3.845731\n",
       "sys_sd           0.812078\n",
       "h_min            1.401941\n",
       "h_max            6.000000\n",
       "h_mean           3.844283\n",
       "h_sd             0.742891\n",
       "N            10000.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute correlation with gold-standard\n",
    "true_eval = Analyzer.metrics_helper(df['true'], df['system'])\n",
    "true_eval.rename(index=metrics_dict, inplace=True)\n",
    "true_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute metrics against aggregated scores\n",
    "def compute_metrics(df_cat):\n",
    "    metrics_n = []\n",
    "    for c in df_cat.columns:\n",
    "        if c.startswith(\"N=\"):\n",
    "            df_metrics = pd.DataFrame(Analyzer.metrics_helper(df_cat[c], df['system'])).transpose()\n",
    "            df_metrics.index = [int(c.split('=')[1])]\n",
    "            df_metrics.index.name = 'N raters'\n",
    "            metrics_n.append(df_metrics)\n",
    "    return pd.concat(metrics_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this computes metrics over 200 raters so can take some time\n",
    "df_metrics = df_agg.groupby('category').apply(compute_metrics).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the metrics and melt the dataframe\n",
    "df_metrics = df_metrics.rename(columns=metrics_dict)\n",
    "df_metrics_long = df_metrics.melt(id_vars=['category', 'N raters'], \n",
    "                                  value_vars=['r', 'R2', 'QWK'],\n",
    "                                  var_name='metrics',\n",
    "                                  value_name='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the plot\n",
    "\n",
    "g=sns.lmplot(x='N raters', y='value', col='metrics', hue='category', data=df_metrics_long,\n",
    "            fit_reg=False, hue_order = config['rater_categories'],\n",
    "            col_order=['r', 'R2', 'QWK'], sharey=False)\n",
    "axes = g.axes[0]\n",
    "\n",
    "for ax in axes:\n",
    "    metric = ax.get_title().split('=')[1].strip()\n",
    "    line_height = true_eval[metric]\n",
    "    ax.axhline(line_height, color='red')\n",
    "    \n",
    "g.savefig(fig_dir / 'intuition.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
